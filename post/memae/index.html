<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>记忆自编码器(MemAE) - Even - A super concise theme for Hugo</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="crazyn2" /><meta name="description" content="主要功能: 异常检测(红框框出图像异常区域) 特征提取(从图像中提取指定的特征) 相对深度自编码器的改进: 提高对异常数据的敏感程度，即两极分化正常" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.110.0 with theme even" />


<link rel="canonical" href="https://crazyn2.github.io/post/memae/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="记忆自编码器(MemAE)" />
<meta property="og:description" content="主要功能: 异常检测(红框框出图像异常区域) 特征提取(从图像中提取指定的特征) 相对深度自编码器的改进: 提高对异常数据的敏感程度，即两极分化正常" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://crazyn2.github.io/post/memae/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-07-29T19:10:05+08:00" />
<meta property="article:modified_time" content="2022-07-29T19:10:05+08:00" />
<meta itemprop="name" content="记忆自编码器(MemAE)">
<meta itemprop="description" content="主要功能: 异常检测(红框框出图像异常区域) 特征提取(从图像中提取指定的特征) 相对深度自编码器的改进: 提高对异常数据的敏感程度，即两极分化正常"><meta itemprop="datePublished" content="2022-07-29T19:10:05+08:00" />
<meta itemprop="dateModified" content="2022-07-29T19:10:05+08:00" />
<meta itemprop="wordCount" content="2582">
<meta itemprop="keywords" content="deep-learning," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="记忆自编码器(MemAE)"/>
<meta name="twitter:description" content="主要功能: 异常检测(红框框出图像异常区域) 特征提取(从图像中提取指定的特征) 相对深度自编码器的改进: 提高对异常数据的敏感程度，即两极分化正常"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Even</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">主页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">标签</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">分类</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Even</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">主页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">标签</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">分类</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">记忆自编码器(MemAE)</h1>

      <div class="post-meta">
        <span class="post-time"> 2022-07-29 </span>
        <div class="post-category">
            <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"> 深度学习 </a>
            </div>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#模型">模型</a>
          <ul>
            <li><a href="#可视化数据">可视化数据</a></li>
            <li><a href="#记忆模块">记忆模块</a></li>
            <li><a href="#损失函数">损失函数</a></li>
          </ul>
        </li>
        <li><a href="#源代码分析">源代码分析</a>
          <ul>
            <li><a href="#数据集">数据集</a></li>
            <li><a href="#模型-1">模型</a></li>
            <li><a href="#hard-shrinkage-for-sparse-addressing稀疏定位硬压缩">hard shrinkage for sparse addressing(稀疏定位硬压缩)</a></li>
            <li><a href="#mnist-数据集训练">mnist 数据集训练</a></li>
            <li><a href="#cifar10">cifar10</a></li>
          </ul>
        </li>
        <li><a href="#数据结果展示">数据结果展示</a></li>
        <li><a href="#当前项目">当前项目</a></li>
        <li><a href="#记忆自编码器无监督转有监督学习">记忆自编码器无监督转有监督学习</a>
          <ul>
            <li><a href="#有监督反向训练损失函数方法">有监督反向训练损失函数方法</a></li>
            <li><a href="#无监督转半监督">无监督转半监督</a></li>
          </ul>
        </li>
        <li><a href="#faq">FAQ</a></li>
        <li><a href="#扩展资料">扩展资料</a></li>
        <li><a href="#个人猜想">个人猜想</a></li>
        <li><a href="#heading"></a></li>
        <li><a href="#heading-1"></a></li>
        <li><a href="#heading-2"></a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>主要功能:</p>
<ul>
<li>异常检测(红框框出图像异常区域)</li>
<li>特征提取(从图像中提取指定的特征)</li>
</ul>
<p>相对深度自编码器的改进: 提高对异常数据的敏感程度，即两极分化正常样本和异常样本的重构误差。</p>
<h2 id="模型">模型</h2>
<ol>
<li>
<p>编码器(Encoder)</p>
<p>$$
z = f_e(x; \theta_e)
$$</p>
<p>$\theta_e$表示 Encoder 网络的权重，$f_e(x; \theta_e)$表示对输入变量 x 进行编码操作，降维输入图像张量</p>
</li>
<li>
<p>记忆模块(Memory Module)</p>
</li>
<li>
<p>解码器(Decoder)
$$
\hat{x} = f_d(\hat{z}; \theta_d)
$$
$\theta_d$表示 Decoder 网络的权重，$f_d(\hat{z}; \theta_d)$表示对输入变量$\hat{z}$进行解码操作，将数据还原成图像</p>
</li>
</ol>
<!-- ![](https://raw.githubusercontent.com/donggong1/memae-anomaly-detection/master/imgs/network.jpg) -->
<h3 id="可视化数据">可视化数据</h3>
<p><a href="https://zhuanlan.zhihu.com/p/369297419">基于 PyTorch 的 PCA 简单实现</a>
<a href="https://zgcr.gitlab.io/2019/03/09/zhu-cheng-fen-fen-xi-pca-jiang-wei-yuan-li-te-zheng-zhi-fen-jie-yu-svd-fen-jie/#toc-heading-7">主成分分析（PCA）降维原理、特征值分解与 SVD 分解</a>
<a href="https://blog.csdn.net/weixin_43844219/article/details/105188844">PCA 的原理和 pytorch 实现</a>
<a href="https://www.cnblogs.com/picassooo/p/12700888.html">Python 实现 PCA 降维</a></p>
<h3 id="记忆模块">记忆模块</h3>
<p>下面具体介绍 MemAE 的记忆模块。</p>
<p>memory 为一个包含 N 个行向量的矩阵$\pmb{M} \in R^{N \times C}$，每个行向量$m_i$表示 M 记忆模块的行。</p>
<p>记忆模块的计算流程如下:</p>
<ol>
<li>编码器输出张量 z 和记忆矩阵 M 内积和 softmax 归一化，输出$\omega$
$$
\omega = \frac{exp(z * m_i)}{\sum_{i=1}^{N} z *m_i}
$$</li>
</ol>
<p>说明: 论文公布的源代码里面没有使用论文描述的余弦相似度，而是输入值和记忆模块内积，再进行 softmax。作者的回答是使用余弦相似度，导致$\omega$全都趋近 0，所以作者认为余弦相似度不适合该模型，所以改用矩阵内积。所以这里把计算公式修改成与源代码一致。
<a href="https://github.com/donggong1/memae-anomaly-detection/issues/12#issuecomment-659951371">https://github.com/donggong1/memae-anomaly-detection/issues/12#issuecomment-659951371</a> 2. 利用设定阈值$\lambda$，对$\omega$进行稀疏化，低于阈值$\lambda$的元素置 0。</p>
<p>$$
\hat{\omega_i} = \frac{max(\omega_i-\lambda,0)\omega_i}{|\omega_i-\lambda|+\varepsilon}
$$</p>
<p>记忆模块的损失函数:</p>
<p>$$
E(\hat{\omega^t})=\sum^{T}_{i=1}-\hat{\omega} *log(\hat{\omega_i})
$$</p>
<p>损失函数是个针对记忆模块 1 的计算结果权重的信息熵，增加$\omega$的稀疏性，限制特征的个数，实现降维的同时，避免不重要信息的影响。</p>
<p>表示一个记忆力元素（memory item），维度为 C，等于编码器输出特征 z 的维度。因此通过记忆网络得到 query 可以表示为：</p>
<p><a href="https://github.com/donggong1/memae-anomaly-detection/issues/17">记忆模块视为神经网络训练参数的一部分</a></p>
<h3 id="损失函数">损失函数</h3>
<p>熵函数取最小，提高记忆模块的稀疏性，增加模型的约束条件，避免过拟合</p>
<p>$$
L\left(\theta_{e}, \theta_{d}, \mathbf{M}\right)=\frac{1}{T} \sum_{t=1}^{T}\left(R\left(\mathbf{x}^{t}, \hat{\mathbf{x}}^{t}\right)+\alpha E\left(\widehat{\mathbf{w}}^{t}\right)\right)
$$</p>
<p>其中$R=|\mathrm{x}-\widehat{\mathrm{x}}|_{2}^{2}$表示重构误差,超参数$ \alpha $原文作者选择的是 0.0002</p>
<h2 id="源代码分析">源代码分析</h2>
<h3 id="数据集">数据集</h3>
<ul>
<li>minist 数据集(图像数据集，手写体，PyTorch 等深度学习框基本自带)</li>
<li>UCSD Anomaly Detection Dataset(视频数据集，视频流的图片都提取出来，<a href="http://www.svcl.ucsd.edu/projects/anomaly/dataset.html">加利福尼亚大学圣迭戈分校提供的异常检测数据集</a>，非步行和异常步行视为异常，比如骑自行车、滑板、开车、轮椅等等)</li>
</ul>
<p>mnist 数据集比较常用，就不介绍了。</p>
<p>UCSD 数据集是固定摄像头记录的行人视频，视频拆分成图片。UCSD 划分为两个子集，对应不同的场景。每个子集划分为训练集和测试集，训练集都是正常样本，测试集中夹杂者异常样本，同时记录异常数据所在图像的区域。
UCSD 数据集预处理:</p>
<ol>
<li>
<p>tif 格式文件转 jpg(tif 和 jpg 是一种图片格式文件)</p>
</li>
<li>
<p>测试集异常图片提取标签</p>
</li>
<li>
<p>设定条件提取视频数据</p>
</li>
</ol>
<h3 id="模型-1">模型</h3>
<p>模型主要架构如开头所示，自编码器的编码器和解码器之间增加一个缓冲区(记忆模块)，自编码器部分流程和神经网络训练基本一致，这里主要分析下记忆模块部分。</p>
<p>记忆模块参数非负</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MemoryUnit</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mem_dim</span><span class="p">,</span> <span class="n">fea_dim</span><span class="p">,</span> <span class="n">shrink_thres</span><span class="o">=</span><span class="mf">0.0025</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">MemoryUnit</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">mem_dim</span> <span class="o">=</span> <span class="n">mem_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fea_dim</span> <span class="o">=</span> <span class="n">fea_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mem_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                             <span class="bp">self</span><span class="o">.</span><span class="n">fea_dim</span><span class="p">))</span>  <span class="c1"># M x C</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">shrink_thres</span> <span class="o">=</span> <span class="n">shrink_thres</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># self.hard_sparse_shrink_opt = nn.Hardshrink(lambd=shrink_thres)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">stdv</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 连续型均匀分布初始化记忆模块参数</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="n">stdv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="n">stdv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">att_weight</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                              <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>  <span class="c1"># Fea x Mem^T, (TxC) x (CxM) = TxM</span>
</span></span><span class="line"><span class="cl">        <span class="n">att_weight</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">att_weight</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># TxM</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ReLU based shrinkage, hard shrinkage for positive value</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shrink_thres</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">att_weight</span> <span class="o">=</span> <span class="n">hard_shrink_relu</span><span class="p">(</span><span class="n">att_weight</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">shrink_thres</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># att_weight = F.softshrink(att_weight, lambd=self.shrink_thres)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># normalize???</span>
</span></span><span class="line"><span class="cl">            <span class="n">att_weight</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">att_weight</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># att_weight = F.softmax(att_weight, dim=1)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># att_weight = self.hard_sparse_shrink_opt(att_weight)</span>
</span></span><span class="line"><span class="cl">        <span class="n">mem_trans</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Mem^T, MxCvvv</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">att_weight</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">mem_trans</span><span class="p">)</span>  <span class="c1"># AttWeight x Mem^T^T = AW x Mem, (TxM) x (MxC) = TxC</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;output&#39;</span><span class="p">:</span> <span class="n">output</span><span class="p">,</span> <span class="s1">&#39;att&#39;</span><span class="p">:</span> <span class="n">att_weight</span><span class="p">}</span>  <span class="c1"># output, att_weight</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="s1">&#39;mem_dim=</span><span class="si">{}</span><span class="s1">, fea_dim=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mem_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fea_dim</span>
</span></span><span class="line"><span class="cl">                                               <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># NxCxHxW -&gt; (NxHxW)xC -&gt; addressing Mem, (NxHxW)xC -&gt; NxCxHxW</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MemModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mem_dim</span><span class="p">,</span> <span class="n">fea_dim</span><span class="p">,</span> <span class="n">shrink_thres</span><span class="o">=</span><span class="mf">0.0025</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">MemModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">mem_dim</span> <span class="o">=</span> <span class="n">mem_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fea_dim</span> <span class="o">=</span> <span class="n">fea_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">shrink_thres</span> <span class="o">=</span> <span class="n">shrink_thres</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">MemoryUnit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mem_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fea_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shrink_thres</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">s</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="n">l</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">l</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">l</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">l</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;wrong feature map size&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_and</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">y_and</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">att</span> <span class="o">=</span> <span class="n">y_and</span><span class="p">[</span><span class="s1">&#39;att&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">l</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">mem_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">l</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">mem_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">l</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">mem_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">y</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;wrong feature map size&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;output&#39;</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;att&#39;</span><span class="p">:</span> <span class="n">att</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># relu based hard shrinkage function, only works for positive values</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">hard_shrink_relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="n">lambd</span><span class="p">)</span> <span class="o">*</span> <span class="nb">input</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="n">lambd</span><span class="p">)</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">epsilon</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="hard-shrinkage-for-sparse-addressing稀疏定位硬压缩">hard shrinkage for sparse addressing(稀疏定位硬压缩)</h3>
<p>提高记忆矩阵稀疏性，设定阈值，矩阵中低于阈值的参数直接设为 0</p>
<p>代码里面没有使用论文中的余弦相似度，而是输入值和记忆模块内积，再进行 softmax，作者的回答是使用余弦相似度，导致记忆模块权重全都趋近 0，所以作者认为余弦相似度不适合该模型，所以改用矩阵内积</p>
<p>为了能够反向传播和简便性，这里使用 ReLU 激活函数筛选矩阵参数
hard_shrink_relu:</p>
<p>$$
\frac{max(\omega_i-\lambda,0)}{|\omega_i-\lambda|+\epsilon}
$$</p>
<h3 id="mnist-数据集训练">mnist 数据集训练</h3>
<p>mem_dim: 记忆模块大小
正常样本标签为 0，异常样本为 1</p>
<h3 id="cifar10">cifar10</h3>
<h2 id="数据结果展示">数据结果展示</h2>
<h2 id="当前项目">当前项目</h2>
<p>需要可视化训练过程</p>
<p>重构误差函数需要进一步改良</p>
<h2 id="记忆自编码器无监督转有监督学习">记忆自编码器无监督转有监督学习</h2>
<p>实际上这是一个错误的改进方法，因为有监督学习不适合异常检测。现实生活中，异常发生机率非常小，数据难采集，而且异常样本不可预测，这些条件直接拒绝了有监督学习的应用。
不过，本人做完了才意识到，所以还是记录下。</p>
<h3 id="有监督反向训练损失函数方法">有监督反向训练损失函数方法</h3>
<p>参考论文: <a href="https://arxiv.org/abs/1906.02694">Deep Semi-Supervised Anomaly Detection</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">@InProceedings{ruff2020deep,
</span></span><span class="line"><span class="cl">  title     = {Deep Semi-Supervised Anomaly Detection},
</span></span><span class="line"><span class="cl">  author    = {Ruff, Lukas and Vandermeulen, Robert A. and G{\&#34;o}rnitz, Nico and Binder, Alexander and M{\&#34;u}ller, Emmanuel and M{\&#34;u}ller, Klaus-Robert and Kloft, Marius},
</span></span><span class="line"><span class="cl">  booktitle = {International Conference on Learning Representations},
</span></span><span class="line"><span class="cl">  year      = {2020},
</span></span><span class="line"><span class="cl">  url       = {https://openreview.net/forum?id=HkgH0TEYwH}
</span></span><span class="line"><span class="cl">}
</span></span></code></pre></td></tr></table>
</div>
</div><p>批处理操作，会对影响 ROC_AUC 指标有影响，原因是使用 PyTorch 自带损失函数，默认是批处理里的数据求和，跟手动编写的函数不一样</p>
<ol>
<li>训练输入异常样本时，原来的损失函数添加负号。发现，这个跟损失函数</li>
<li>异常样本输入，损失函数取倒数</li>
</ol>
<h3 id="无监督转半监督">无监督转半监督</h3>
<h2 id="faq">FAQ</h2>
<p>Python pytorch 多进程运行</p>
<h2 id="扩展资料">扩展资料</h2>
<p><a href="https://icode.best/i/25227345995644">opencv 和 pillow 比较</a></p>
<h2 id="个人猜想">个人猜想</h2>
<ul>
<li>
<p>不在编码器和解码器之间增加记忆模块，而是在其他地方增加，是否会取得更好的效果</p>
</li>
<li>
<p>编码器之间增加 skip connection</p>
</li>
<li>
<p>中间输入增加随机噪声</p>
</li>
<li>
<p>新模型的改进不会只拘泥于一个旧模型，可以是其他模型，比如降噪自编码器的功能是剔除原有数据的噪声，通过无监督学习的方法，这个功能在原有自编码器功能上是没有，完全是基于功能进行改进的</p>
</li>
<li>
<p>编码器中间还是结尾增加 SVDD 模块</p>
</li>
</ul>






<div style="width: 100%;height: 300px;margin: 0 auto">
    <canvas id="6281dcc44dacccae58ed5cbf44dffc8b"></canvas>
</div>
<script src="https://cdn.jsdelivr.net/npm/chart.js/dist/chart.min.js"></script>
<script type="text/javascript">
    var ctx = document.getElementById('6281dcc44dacccae58ed5cbf44dffc8b').getContext('2d');
    var options =  {
type: 'line',
data: { labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], datasets: [{ label: 'mse_roc_auc', data: [0.695830, 0.502663, 0.661373, 0.598149, 0.666568, 0.641934,0.549041, 0.488391, 0.762655,0.441765 ], backgroundColor: 'rgba(255, 99, 132, 0.2)',
borderColor: 'rgba(255, 99, 132, 1)',
borderWidth: 1 }] },
options: { maintainAspectRatio: false, scales: { yAxes: [{ ticks: { beginAtZero: true } }] } } };
    new Chart(ctx, options);
</script>

<h2 id="heading"></h2>
<p>pretrain 只用了 mse，训练的时候增加了记忆模块和 svdd
test 只用了 mse</p>
<h2 id="heading-1"></h2>
<p>r6. 经历了 3 次碰撞， 0.048ms
p8. a. (1-1/N)^(N-1),. p=1/N b. 1/e=0.37
p9. max(p(1-p)^2(N-1))-1/2e</p>
<h2 id="heading-2"></h2>
<p>使用自己的网络模型，分别测试 svdd、autoencoder、mvdd 等方法
IAED 的网络模型和 Deep SVDD 的网络模型不一样</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">crazyn2</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2022-07-29
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/deep-learning/">deep-learning</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/multi/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Pytorch多进程训练</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
      <a href="http://localhost:1313" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="http://localhost:1313" class="iconfont icon-twitter" title="twitter"></a>
      <a href="http://localhost:1313" class="iconfont icon-facebook" title="facebook"></a>
      <a href="http://localhost:1313" class="iconfont icon-linkedin" title="linkedin"></a>
      <a href="http://localhost:1313" class="iconfont icon-google" title="google"></a>
      <a href="http://localhost:1313" class="iconfont icon-github" title="github"></a>
      <a href="http://localhost:1313" class="iconfont icon-weibo" title="weibo"></a>
      <a href="http://localhost:1313" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="http://localhost:1313" class="iconfont icon-douban" title="douban"></a>
      <a href="http://localhost:1313" class="iconfont icon-pocket" title="pocket"></a>
      <a href="http://localhost:1313" class="iconfont icon-tumblr" title="tumblr"></a>
      <a href="http://localhost:1313" class="iconfont icon-instagram" title="instagram"></a>
      <a href="http://localhost:1313" class="iconfont icon-gitlab" title="gitlab"></a>
      <a href="http://localhost:1313" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="https://crazyn2.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2023<span class="heart"><i class="iconfont icon-heart"></i></span><span>crazyn2</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
